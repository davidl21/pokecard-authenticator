{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authentic Pokemon Card Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from torchvision.transforms import transforms \n",
    "from torch.utils.data import DataLoader \n",
    "from torch.optim import Adam\n",
    "from torch.optim import SGD\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: for optimizer, test on Adam, but perhaps try SGD as the optimizer as it may generalize better in certain cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device selection, much better to train on cuda\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transforms.Compose([\n",
    "    # ensure that all data is 256x256 input\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((256, 256)),\n",
    "    # increase variation, increase # of unique images by a factor of 2\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    \n",
    "    transforms.Normalize([0.5, 0.5, 0.5], \n",
    "                        [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "#transformer = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyeperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "learning_rate = 0.001\n",
    "\n",
    "# 0: Fake, 1: Real\n",
    "num_classes = 2\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets and Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealAndFakeCards(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, ds_class, transform=None):\n",
    "        # read in CSV file, with id and label. \n",
    "        csv_path = os.path.join(root_dir, csv_file)\n",
    "        self.annotations = pd.read_csv(csv_path)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.ds_class = ds_class\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        directory = os.path.join(self.root_dir, self.ds_class)\n",
    "        img_path = directory + '/' + '{}.jpg'.format(self.annotations.iloc[i, 0])\n",
    "        image = io.imread(img_path)\n",
    "        y_label = torch.tensor(int(self.annotations.iloc[i, 1]))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "train_ds = RealAndFakeCards(csv_file='train_labels.csv', root_dir='pcard_dataset', ds_class='train', transform=transformer)\n",
    "test_ds = RealAndFakeCards(csv_file='test_labels.csv', root_dir='pcard_dataset', ds_class='test', transform=transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0.3647, 0.3647, 0.3647,  ..., 0.4824, 0.4824, 0.4824],\n",
      "         [0.3647, 0.3725, 0.3725,  ..., 0.4824, 0.4824, 0.4902],\n",
      "         [0.3804, 0.3804, 0.3804,  ..., 0.4902, 0.4902, 0.4902],\n",
      "         ...,\n",
      "         [0.2706, 0.2706, 0.2784,  ..., 0.4118, 0.4118, 0.4118],\n",
      "         [0.2706, 0.2706, 0.2706,  ..., 0.4118, 0.4039, 0.4039],\n",
      "         [0.2706, 0.2706, 0.2706,  ..., 0.4039, 0.4039, 0.4039]],\n",
      "\n",
      "        [[0.3490, 0.3490, 0.3490,  ..., 0.4667, 0.4667, 0.4667],\n",
      "         [0.3490, 0.3569, 0.3569,  ..., 0.4667, 0.4667, 0.4745],\n",
      "         [0.3647, 0.3647, 0.3647,  ..., 0.4745, 0.4745, 0.4745],\n",
      "         ...,\n",
      "         [0.2627, 0.2627, 0.2706,  ..., 0.4039, 0.4039, 0.4039],\n",
      "         [0.2627, 0.2627, 0.2627,  ..., 0.4039, 0.3961, 0.3961],\n",
      "         [0.2627, 0.2627, 0.2627,  ..., 0.3961, 0.3961, 0.3961]],\n",
      "\n",
      "        [[0.3569, 0.3569, 0.3569,  ..., 0.4745, 0.4745, 0.4745],\n",
      "         [0.3569, 0.3647, 0.3647,  ..., 0.4745, 0.4745, 0.4824],\n",
      "         [0.3725, 0.3725, 0.3725,  ..., 0.4824, 0.4824, 0.4824],\n",
      "         ...,\n",
      "         [0.2235, 0.2235, 0.2314,  ..., 0.3647, 0.3647, 0.3647],\n",
      "         [0.2235, 0.2235, 0.2235,  ..., 0.3647, 0.3569, 0.3569],\n",
      "         [0.2235, 0.2235, 0.2235,  ..., 0.3569, 0.3569, 0.3569]]]), tensor(0))\n"
     ]
    }
   ],
   "source": [
    "print(test_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loaders\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['0', '1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=num_classes):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        # output after convolution filter = \n",
    "        # ((256 - 3 + 2(1))/1) + 1\n",
    "\n",
    "        # input shape = (batch_size=16, rgb_channels=3, h=256, w=256)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        # shape = (16, 12, 256, 256)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=12)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=20, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=20, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=32)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.fc = nn.Linear(in_features=32*128*128, out_features=num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.conv1(x)\n",
    "        output = self.bn1(output)\n",
    "        output = self.relu1(output)\n",
    "        output = self.pool(output)\n",
    "\n",
    "        output = self.conv2(output)\n",
    "        output = self.relu2(output)\n",
    "        \n",
    "        output = self.conv3(output)\n",
    "        output = self.bn3(output)\n",
    "        output = self.relu3(output)\n",
    "\n",
    "        output = output.view(-1, 32*128*128)\n",
    "\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(num_classes=len(classes)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "#optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=0.00) # Adam\n",
    "optimizer = SGD(model.parameters(), lr=learning_rate) #SGD\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of training data and test data, as per dataset documentation\n",
    "train_count = 373\n",
    "test_count = 78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0\n",
      "Epoch #1\n",
      "Epoch #2\n",
      "Epoch #3\n",
      "Epoch #4\n",
      "Epoch #5\n",
      "Epoch #6\n",
      "Epoch #7\n",
      "Epoch #8\n",
      "Epoch #9\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch #{}\".format(epoch))\n",
    "    for i, (images, labels) in enumerate(train_dl):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "\n",
    "        # back propogation \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.87179487179486 percent accurate.\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = 0.0\n",
    "with torch.no_grad():\n",
    "    for i, (images, labels) in enumerate(test_dl):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs=model(images)\n",
    "        _, prediction = torch.max(outputs, 1)\n",
    "        test_accuracy += int(torch.sum(prediction==labels.data))\n",
    "    \n",
    "    test_accuracy=test_accuracy/test_count\n",
    "    \n",
    "    test_accuracy *= 100\n",
    "print('{} percent accurate.'.format(test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
